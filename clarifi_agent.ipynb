{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Agents Lab Notebook v1.0.0\nThis notebook contains steps and code to demonstrate the use of agents\nconfigured in Agent Lab in watsonx.ai. It introduces Python API commands\nfor authentication using API key and invoking a LangGraph agent with a watsonx chat model.\n\n**Note:** Notebook code generated using Agent Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Creating an agent with a set of tools using a specified model and parameters\n* Invoking the agent to generate a response \n\n# Setup"}, {"metadata": {}, "cell_type": "code", "source": "# import dependencies\nfrom langchain_ibm import ChatWatsonx\nfrom ibm_watsonx_ai import APIClient\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\nimport json\nimport requests", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Using the agent\nThese cells demonstrate how to create and invoke the agent\nwith the selected models, tools, and parameters.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:"}, {"metadata": {}, "cell_type": "code", "source": "model_id = \"ibm/granite-3-3-8b-instruct\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:"}, {"metadata": {}, "cell_type": "code", "source": "parameters = {\n    \"frequency_penalty\": 0,\n    \"max_tokens\": 2000,\n    \"presence_penalty\": 0,\n    \"temperature\": 0,\n    \"top_p\": 1\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:"}, {"metadata": {}, "cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Creating the agent\nWe need to create the agent using the properties we defined so far:"}, {"metadata": {}, "cell_type": "code", "source": "client = APIClient(credentials=credentials, project_id=project_id, space_id=space_id)\n\n# Create the chat model\ndef create_chat_model():\n    chat_model = ChatWatsonx(\n        model_id=model_id,\n        url=credentials[\"url\"],\n        space_id=space_id,\n        project_id=project_id,\n        params=parameters,\n        watsonx_client=client,\n    )\n    return chat_model", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\n\ndef decrypt_tool_secrets(secrets):\n    url = \"https://api.dataplatform.cloud.ibm.com\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f'Bearer {context.generate_token()}'\n    }\n\n    body = {\n        \"secrets\": secrets,\n        \"project_id\": project_id\n    }\n\n    response = requests.post(f'{url}/wx/v1-beta/utility_agent_tools/secret/decrypt', headers=headers, json=body)\n\n    return response.json().get(\"secrets\")\n\nencrypted_secrets = [\n    \"gcm-agent-tools-qHi31me0EfjVZVuGAnau05GBdpyvCVyV:BOWgED2z8YlNmtUMwqxldQ==;Gf0ORCtOCWWhkYdqgA3cVQ==:MtjXLZI84d/xxSjH54b8vV3wgPZZVRWU9OF8fU1NLYcBMNl2pjfbwtu5b5Y=\"\n]\ndecrypted_secrets = decrypt_tool_secrets(encrypted_secrets)\n\nTavilySearch_apiKey = decrypted_secrets[0]\n\n\n\ndef create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n    from langchain_core.tools import StructuredTool\n    utility_agent_tool = Toolkit(\n        api_client=api_client\n    ).get_tool(tool_name)\n\n    tool_description = utility_agent_tool.get(\"description\")\n\n    if (kwargs.get(\"tool_description\")):\n        tool_description = kwargs.get(\"tool_description\")\n    elif (utility_agent_tool.get(\"agent_description\")):\n        tool_description = utility_agent_tool.get(\"agent_description\")\n    \n    tool_schema = utility_agent_tool.get(\"input_schema\")\n    if (tool_schema == None):\n        tool_schema = {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"properties\": {\n                \"input\": {\n                    \"description\": \"input for the tool\",\n                    \"type\": \"string\"\n                }\n            }\n        }\n    \n    def run_tool(**tool_input):\n        query = tool_input\n        if (utility_agent_tool.get(\"input_schema\") == None):\n            query = tool_input.get(\"input\")\n\n        results = utility_agent_tool.run(\n            input=query,\n            config=params\n        )\n        \n        return results.get(\"output\")\n    \n    return StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=run_tool,\n        args_schema=tool_schema\n    )\n\n\ndef create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n    from langchain_core.tools import StructuredTool\n    import ast\n\n    def call_tool(**kwargs):\n        tree = ast.parse(tool_code, mode=\"exec\")\n        custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n        function_name = custom_tool_functions[0].name\n        compiled_code = compile(tree, 'custom_tool', 'exec')\n        namespace = tool_params if tool_params else {}\n        exec(compiled_code, namespace)\n        return namespace[function_name](**kwargs)\n        \n    tool = StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=call_tool,\n        args_schema=tool_schema\n    )\n    return tool\n\ndef create_custom_tools():\n    custom_tools = []\n\n\ndef create_tools(context):\n    tools = []\n    \n    config = None\n    tools.append(create_utility_agent_tool(\"GoogleSearch\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, client))\n    config = {\n        \"maxResults\": 5\n    }\n    tools.append(create_utility_agent_tool(\"Wikipedia\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"WebCrawler\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"Weather\", config, client))\n    config = {\n        \"maxResults\": 10,\n        \"apiKey\": TavilySearch_apiKey\n    }\n    tools.append(create_utility_agent_tool(\"TavilySearch\", config, client))\n\n    return tools", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def create_agent(context):\n    # Initialize the agent\n    chat_model = create_chat_model()\n    tools = create_tools(context)\n\n    memory = MemorySaver()\n    instructions = \"\"\"The agent should begin by scanning the uploaded research paper and identifying its key sections, including the abstract, methodology, results, and conclusion. It must extract and condense these sections into a concise summary using clear, non-technical language while preserving the original meaning. Any jargon should be simplified or briefly explained. The agent must structure the summary in logical order, with optional headings such as \u201cPurpose,\u201d \u201cKey Findings,\u201d and \u201cImplications.\u201d It should adapt to different academic domains\u2014such as engineering, life sciences, or social sciences\u2014by adjusting the tone and depth of explanation accordingly. The output should be suitable for both academic and general audiences, depending on user preference. Multilingual support and accessibility features must be included to ensure usability across diverse learning environments. Lastly, it should maintain academic ethics by avoiding plagiarism, paraphrasing responsibly, and citing the source clearly when needed.\n\nTo deliver consistently simplified and educationally valuable content, the agent should start by detecting the subject matter and identifying core concepts, avoiding unnecessary complexity unless requested. It must paraphrase original academic material into clear, plain language while maintaining factual accuracy. The agent should structure output logically, using concise sections, bullets, or numbered lists when appropriate to aid readability. Users may submit diverse content formats, so the agent must flexibly adapt\u2014whether it\u2019s a topic, document, or question\u2014while tailoring responses to user level, whether secondary education or higher. It must respect user preferences by supporting multiple languages and enabling accessibility for screen readers or neurodiverse learners. The agent must uphold ethical standards, providing original rewording without copying and crediting any sourced material. Lastly, when feasible, it should offer varied output styles, such as simplified summaries, step-by-step guides, FAQs, or glossaries, and allow users to choose how they want information presented for effective and inclusive learning.\n\n\nYou are a helpful assistant that uses tools to answer questions in detail.\nWhen greeted, say \\\"Hi, I am CLARIFI. How can I help you?\\\"\n\n1. Identify and Extract Key Concepts  \n   - Focus on core ideas from academic materials.  \n   - Ignore non-essential details, footnotes, or overly technical examples unless requested.\n\n2. Simplify Language without Losing Meaning \n   - Use plain, everyday language while retaining academic accuracy.  \n   - Replace technical jargon with familiar terms or provide short definitions.\n\n3. Segment Content Logically \n   - Break down long paragraphs into digestible sections.  \n   - Use headings, bullet points, and numbered lists to enhance clarity.\n\n4. Adapt Output Based on User Input\n   - Respond differently based on whether the input is a topic, textbook, or paper.  \n   - Adjust tone and depth for high school vs college-level material.\n\n5. Support Multilingual Outputs  \n   - Automatically detect and offer translations in user-preferred languages.  \n   - Maintain content accuracy across translations.\n\n6. Maintain Accessibility\n   - Ensure content is readable on small screens and compatible with text-to-speech tools.  \n   - Prioritize formats friendly to neurodiverse learners (e.g. concise summaries, minimal distractions).\n\n7. Respect Academic Integrity\n   - Paraphrase content ethically\u2014no direct copying from copyrighted sources.  \n   - Acknowledge referenced sources if summarizing published research.\n\n8. Offer Multiple Output Styles (when feasible)  \n   - Summary  \n   - Step-by-step explanation  \n   - FAQ format  \n   - Glossary of simplified terms\n\"\"\"\n\n    agent = create_react_agent(chat_model, tools=tools, checkpointer=memory, state_modifier=instructions)\n\n    return agent", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualize the graph\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\nImage(\n    create_agent(context).get_graph().draw_mermaid_png(\n        draw_method=MermaidDrawMethod.API,\n    )\n)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Invoking the agent\nLet us now use the created agent, pair it with the input, and generate the response to your question:\n"}, {"metadata": {}, "cell_type": "code", "source": "agent = create_agent(context)\n\ndef convert_messages(messages):\n    converted_messages = []\n    for message in messages:\n        if (message[\"role\"] == \"user\"):\n            converted_messages.append(HumanMessage(content=message[\"content\"]))\n        elif (message[\"role\"] == \"assistant\"):\n            converted_messages.append(AIMessage(content=message[\"content\"]))\n    return converted_messages\n\nquestion = input(\"Question: \")\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": question\n}]\n\ngenerated_response = agent.invoke(\n    { \"messages\": convert_messages(messages) },\n    { \"configurable\": { \"thread_id\": \"42\" } }\n)\n\nprint_full_response = False\n\nif (print_full_response):\n    print(generated_response)\nelse:\n    result = generated_response[\"messages\"][-1].content\n    print(f\"Agent: {result}\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}